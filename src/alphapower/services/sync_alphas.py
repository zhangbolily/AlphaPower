"""
åŒæ­¥å› å­æ•°æ®æ¨¡å—ã€‚

è¯¥æ¨¡å—æä¾›äº†ä» AlphaPower API åŒæ­¥å› å­æ•°æ®åˆ°æ•°æ®åº“çš„åŠŸèƒ½ï¼Œæ”¯æŒå…¨é‡å’Œå¢é‡åŒæ­¥ã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. è·å–å› å­æ•°æ®å¹¶å¤„ç†ï¼ˆæ”¯æŒå¹¶è¡Œåˆ†ç‰‡å¤„ç†ï¼‰ã€‚
2. å°†å› å­æ•°æ®æ’å…¥æˆ–æ›´æ–°åˆ°æ•°æ®åº“ã€‚
3. æ”¯æŒå› å­åˆ†ç±»å’Œç«èµ›æ•°æ®çš„å…³è”å¤„ç†ã€‚
4. æä¾›æ—¥å¿—è®°å½•ï¼Œæ”¯æŒè°ƒè¯•ã€ä¿¡æ¯ã€è­¦å‘Šå’Œé”™è¯¯çº§åˆ«çš„æ—¥å¿—è¾“å‡ºã€‚

æ¨¡å—ç‰¹ç‚¹ï¼š
- ä½¿ç”¨å¼‚æ­¥ IO æé«˜æ•°æ®åŒæ­¥æ•ˆç‡ã€‚
- æ”¯æŒé€šè¿‡ä¿¡å·å¤„ç†å™¨ä¼˜é›…åœ°ç»ˆæ­¢åŒæ­¥æ“ä½œã€‚
- æä¾›è¯¦ç»†çš„æ—¥å¿—è®°å½•ï¼Œä¾¿äºé—®é¢˜æ’æŸ¥å’Œæ€§èƒ½ç›‘æ§ã€‚
"""

import asyncio
import signal
import types
from datetime import datetime, timedelta
from typing import Any, List, Optional, Tuple

from structlog.stdlib import BoundLogger

from alphapower.client import (
    AlphaView,
    ClassificationView,
    CompetitionRefView,
    RegularView,
    SelfAlphaListQueryParams,
    SelfAlphaListView,
    WorldQuantClient,
    wq_client,
)
from alphapower.constants import Color, Database, Grade
from alphapower.dal.alphas import (
    AlphaDAL,
    ClassificationDAL,
    CompetitionDAL,
)
from alphapower.dal.base import DALFactory
from alphapower.entity import (
    Alpha,
    Classification,
    Competition,
    Regular,
    Setting,
)
from alphapower.internal.db_session import get_db_session
from alphapower.internal.logging import get_logger

from .sync_competition import competition_data_expire_check, sync_competition
from .utils import create_sample

# é…ç½®æ—¥å¿—
console_logger: BoundLogger = get_logger(__name__, enable_console=True)
file_logger: BoundLogger = get_logger(__name__, enable_console=False)

# TODO(Ball Chang): æ”¯æŒå…¨é‡å’Œå¢é‡åŒæ­¥ï¼ŒåŠªåŠ›æé«˜æ•°æ®åŒæ­¥å¹¶å‘åº¦å’Œå†™å…¥æ€§èƒ½
# TODO(Ball Chang): æ‰¾ä¸€ä¸ªå¥½çš„è§£å†³æ–¹æ¡ˆæ¥åˆ¤æ–­å› å­å›æµ‹é…ç½®æ˜¯å¦ç›¸åŒ

# å…¨å±€äº‹ä»¶ï¼Œç”¨äºé€šçŸ¥æ‰€æœ‰åç¨‹ç»ˆæ­¢æ“ä½œ
exit_event: asyncio.Event = asyncio.Event()


def create_alphas_settings(alpha_data: AlphaView) -> Setting:
    """
    åˆ›å»º AlphaSettings å®ä¾‹ã€‚

    Args:
        alpha_data (AlphaView): åŒ…å«å› å­è®¾ç½®ä¿¡æ¯çš„æ•°æ®å¯¹è±¡ã€‚

    Returns:
        Setting: åˆ›å»ºçš„å› å­è®¾ç½®å®ä¾‹ã€‚

    Raises:
        AttributeError: å¦‚æœ alpha_data ä¸­ç¼ºå°‘å¿…è¦çš„å­—æ®µã€‚

    è¯´æ˜:
        è¯¥å‡½æ•°å°† AlphaView ä¸­çš„è®¾ç½®ä¿¡æ¯æå–å¹¶è½¬æ¢ä¸º Setting å®ä¾‹ã€‚
        ä¸»è¦ç”¨äºå› å­æ•°æ®çš„æ ‡å‡†åŒ–å¤„ç†ã€‚
    """
    try:
        return Setting(
            # æå–å› å­è®¾ç½®ä¸­çš„å„ä¸ªå­—æ®µ
            instrument_type=alpha_data.settings.instrument_type,
            region=alpha_data.settings.region,
            universe=alpha_data.settings.universe,
            delay=alpha_data.settings.delay,
            decay=alpha_data.settings.decay,
            neutralization=alpha_data.settings.neutralization,
            truncation=alpha_data.settings.truncation,
            pasteurization=alpha_data.settings.pasteurization,
            unit_handling=alpha_data.settings.unit_handling,
            nan_handling=alpha_data.settings.nan_handling,
            language=alpha_data.settings.language,
            visualization=alpha_data.settings.visualization,
            test_period=getattr(alpha_data.settings, "test_period", None),  # å¯é€‰å­—æ®µ
        )
    except AttributeError as e:
        raise AttributeError(f"å› å­æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {e}") from e


def create_alphas_regular(regular: RegularView) -> Regular:
    """
    åˆ›å»º AlphaRegular å®ä¾‹ã€‚

    Args:
        regular (RegularView): åŒ…å«å› å­è§„åˆ™è¯¦ç»†ä¿¡æ¯çš„å¯¹è±¡ã€‚

    Returns:
        Regular: åˆ›å»ºçš„å› å­è§„åˆ™å®ä¾‹ã€‚

    è¯´æ˜:
        è¯¥å‡½æ•°å°† RegularView ä¸­çš„è§„åˆ™ä¿¡æ¯æå–å¹¶è½¬æ¢ä¸º Regular å®ä¾‹ã€‚
        ä¸»è¦ç”¨äºå› å­è§„åˆ™çš„æ ‡å‡†åŒ–å¤„ç†ã€‚
    """
    return Regular(
        code=regular.code,  # å› å­è§„åˆ™ä»£ç 
        description=getattr(regular, "description", None),  # å¯é€‰æè¿°
        operator_count=regular.operator_count,  # æ“ä½œç¬¦æ•°é‡
    )


async def create_alpha_classifications(
    classifications_data: Optional[List[ClassificationView]],
) -> List[Classification]:
    """
    åˆ›å»ºæˆ–è·å– Classification å®ä¾‹åˆ—è¡¨ã€‚

    Args:
        classifications_data (Optional[List[ClassificationView]]): åˆ†ç±»æ•°æ®åˆ—è¡¨ã€‚

    Returns:
        List[Classification]: Classification å®ä¾‹åˆ—è¡¨ã€‚

    è¯´æ˜:
        è¯¥å‡½æ•°ä¼šæ ¹æ®åˆ†ç±»æ•°æ®åˆ›å»ºæˆ–æ›´æ–°æ•°æ®åº“ä¸­çš„åˆ†ç±»è®°å½•ã€‚
        å¦‚æœåˆ†ç±»æ•°æ®ä¸ºç©ºï¼Œåˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    if classifications_data is None:
        return []

    entity_objs: List[Classification] = []

    async with get_db_session(Database.ALPHAS) as session:
        # ä½¿ç”¨ DALFactory åˆ›å»º DAL å®ä¾‹
        classification_dal: ClassificationDAL = DALFactory.create_dal(
            ClassificationDAL, session
        )

        for data in classifications_data:
            classification = Classification(
                classification_id=data.id,  # åˆ†ç±»å”¯ä¸€æ ‡è¯†
                name=data.name,  # åˆ†ç±»åç§°
            )

            # æ ¹æ®å”¯ä¸€é”®æ’å…¥æˆ–æ›´æ–°åˆ†ç±»è®°å½•
            classification = await classification_dal.upsert_by_unique_key(
                classification, "classification_id"
            )
            entity_objs.append(classification)

    return entity_objs


async def query_alpha_competitions(
    competitions_data: Optional[List[CompetitionRefView]],
) -> List[Competition]:
    """
    åˆ›å»ºæˆ–è·å– AlphaCompetition å®ä¾‹åˆ—è¡¨ã€‚

    Args:
        competitions_data (Optional[List[CompetitionRefView]]): æ¯”èµ›æ•°æ®åˆ—è¡¨ã€‚

    Returns:
        List[Competition]: Competition å®ä¾‹åˆ—è¡¨ã€‚

    Raises:
        ValueError: å¦‚æœæœªæ‰¾åˆ°ä»»ä½•æ¯”èµ›æ•°æ®ã€‚
        RuntimeError: å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ã€‚

    è¯´æ˜:
        è¯¥å‡½æ•°ä¼šæ ¹æ®æ¯”èµ›æ•°æ®æŸ¥è¯¢æ•°æ®åº“ä¸­çš„æ¯”èµ›è®°å½•ã€‚
        å¦‚æœæœªæ‰¾åˆ°ä»»ä½•æ¯”èµ›è®°å½•ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸ã€‚
    """
    if competitions_data is None:
        return []

    competition_ids: List[str] = [
        competition.id for competition in competitions_data if competition.id
    ]

    try:
        async with get_db_session(Database.ALPHAS) as session:
            # ä½¿ç”¨ DALFactory åˆ›å»º DAL å®ä¾‹
            competition_dal: CompetitionDAL = DALFactory.create_dal(
                CompetitionDAL, session
            )

            entity_objs: List[Competition] = await competition_dal.find_by(
                in_={"competition_id": competition_ids}
            )
            if not entity_objs:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¯”èµ›æ•°æ®ï¼Œè¯·æ£€æŸ¥æ¯”èµ›æ•°æ®æ˜¯å¦æ­£ç¡®ã€‚")
    except Exception as e:
        raise RuntimeError(f"æŸ¥è¯¢æ¯”èµ›æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}") from e

    return entity_objs


def create_alphas(
    alpha_data: AlphaView,
    settings: Setting,
    regular: Regular,
    classifications: List[Classification],
    competitions: List[Competition],
) -> Alpha:
    """
    åˆ›å»º Alpha å®ä¾‹ã€‚

    Args:
        alpha_data (AlphaView): åŒ…å«å› å­è¯¦ç»†ä¿¡æ¯çš„å¯¹è±¡ã€‚
        settings (Setting): å› å­è®¾ç½®å®ä¾‹ã€‚
        regular (Regular): å› å­è§„åˆ™å®ä¾‹ã€‚
        classifications (List[Classification]): åˆ†ç±»å®ä¾‹åˆ—è¡¨ã€‚
        competitions (List[Competition]): æ¯”èµ›å®ä¾‹åˆ—è¡¨ã€‚

    Returns:
        Alpha: åˆ›å»ºçš„ Alpha å®ä¾‹ã€‚

    è¯´æ˜:
        è¯¥å‡½æ•°å°†å› å­æ•°æ®ã€è®¾ç½®ã€è§„åˆ™ã€åˆ†ç±»å’Œæ¯”èµ›ä¿¡æ¯æ•´åˆä¸ºä¸€ä¸ª Alpha å®ä¾‹ã€‚
        ä¸»è¦ç”¨äºå› å­æ•°æ®çš„æ ‡å‡†åŒ–å¤„ç†ã€‚
    """
    return Alpha(
        alpha_id=alpha_data.id,  # å› å­å”¯ä¸€æ ‡è¯†
        type=alpha_data.type,  # å› å­ç±»å‹
        author=alpha_data.author,  # ä½œè€…ä¿¡æ¯
        settings=settings,  # å› å­è®¾ç½®
        regular=regular,  # å› å­è§„åˆ™
        date_created=alpha_data.date_created,  # åˆ›å»ºæ—¥æœŸ
        date_submitted=getattr(alpha_data, "date_submitted", None),  # æäº¤æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        date_modified=alpha_data.date_modified,  # ä¿®æ”¹æ—¥æœŸ
        name=getattr(alpha_data, "name", None),  # å› å­åç§°ï¼ˆå¯é€‰ï¼‰
        favorite=alpha_data.favorite,  # æ˜¯å¦æ”¶è—
        hidden=alpha_data.hidden,  # æ˜¯å¦éšè—
        color=alpha_data.color if alpha_data.color else Color.NONE,  # å› å­é¢œè‰²
        category=getattr(alpha_data, "category", None),  # å› å­ç±»åˆ«ï¼ˆå¯é€‰ï¼‰
        tags=alpha_data.tags,  # å› å­æ ‡ç­¾
        grade=alpha_data.grade if alpha_data.grade else Grade.DEFAULT,  # å› å­ç­‰çº§
        stage=alpha_data.stage,  # å› å­é˜¶æ®µ
        status=alpha_data.status,  # å› å­çŠ¶æ€
        in_sample=create_sample(alpha_data.in_sample),  # æ ·æœ¬å†…æ•°æ®
        out_sample=create_sample(alpha_data.out_sample),  # æ ·æœ¬å¤–æ•°æ®
        train=create_sample(alpha_data.train),  # è®­ç»ƒæ•°æ®
        test=create_sample(alpha_data.test),  # æµ‹è¯•æ•°æ®
        prod=create_sample(alpha_data.prod),  # ç”Ÿäº§æ•°æ®
        competitions=competitions,  # å…³è”çš„æ¯”èµ›
        classifications=classifications,  # å…³è”çš„åˆ†ç±»
        themes=",".join(alpha_data.themes) if alpha_data.themes else None,  # ä¸»é¢˜
        # TODO(Ball Chang): pyramids å­—æ®µéœ€è¦é‡æ–°è®¾è®¡
        pyramids=None,
        team=",".join(alpha_data.team) if alpha_data.team else None,  # å›¢é˜Ÿä¿¡æ¯
    )


async def fetch_last_sync_time_range(
    client: WorldQuantClient,
) -> Tuple[datetime, datetime]:
    """
    è·å–ä¸Šæ¬¡åŒæ­¥çš„æ—¶é—´èŒƒå›´ã€‚

    Args:
        client (WorldQuantClient): å®¢æˆ·ç«¯å®ä¾‹ã€‚

    Returns:
        Tuple[datetime, datetime]: ä¸Šæ¬¡åŒæ­¥çš„å¼€å§‹å’Œç»“æŸæ—¶é—´ã€‚

    Raises:
        RuntimeError: å¦‚æœæ•°æ®åº“æŸ¥è¯¢æˆ– API è¯·æ±‚å¤±è´¥ã€‚

    è¯´æ˜:
        è¯¥å‡½æ•°ä¼šä»æ•°æ®åº“æˆ– API è·å–æœ€è¿‘çš„å› å­åŒæ­¥æ—¶é—´èŒƒå›´ã€‚
    """
    await file_logger.adebug(
        "è¿›å…¥ fetch_last_sync_time_range å‡½æ•°", client=str(client), emoji="ğŸ”"
    )

    try:
        async with get_db_session(Database.ALPHAS) as session:
            alpha_dal: AlphaDAL = DALFactory.create_dal(AlphaDAL, session)
            last_alpha: Optional[Alpha] = await alpha_dal.find_one_by(
                order_by=Alpha.date_created.desc(),
            )

            start_time: datetime
            end_time: datetime = datetime.now()

            if last_alpha:
                start_time = last_alpha.date_created
                await file_logger.adebug(
                    "æ‰¾åˆ°æœ€è¿‘çš„å› å­è®°å½•",
                    last_alpha_id=last_alpha.alpha_id,
                    last_alpha_date_created=last_alpha.date_created,
                    emoji="ğŸ“…",
                )
            else:
                query_params: SelfAlphaListQueryParams = SelfAlphaListQueryParams(
                    limit=1,
                    offset=0,
                    order="dateCreated",
                )

                alphas_data_result: SelfAlphaListView = (
                    await client.alpha_get_self_list(query=query_params)
                )

                if alphas_data_result.count > 0:
                    start_time = alphas_data_result.results[0].date_created
                    await file_logger.adebug(
                        "ä» API è·å–æœ€è¿‘çš„å› å­è®°å½•",
                        api_result_count=alphas_data_result.count,
                        start_time=start_time,
                        emoji="ğŸŒ",
                    )
                else:
                    start_time = datetime.now()
                    await file_logger.awarning(
                        "æœªæ‰¾åˆ°ä»»ä½•å› å­è®°å½•ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºå¼€å§‹æ—¶é—´",
                        start_time=start_time,
                        emoji="âš ï¸",
                    )
    except Exception as e:
        raise RuntimeError(f"è·å–åŒæ­¥æ—¶é—´èŒƒå›´æ—¶å‘ç”Ÿé”™è¯¯: {e}") from e

    await file_logger.adebug(
        "é€€å‡º fetch_last_sync_time_range å‡½æ•°",
        start_time=start_time,
        end_time=end_time,
        emoji="âœ…",
    )
    return start_time, end_time


async def process_alphas_page(alphas_results: List[AlphaView]) -> Tuple[int, int]:
    """
    å¼‚æ­¥å¤„ç†å•é¡µ alphas æ•°æ®ã€‚

    Args:
        alphas_results (List[AlphaView]): è¦å¤„ç†çš„å› å­æ•°æ®åˆ—è¡¨ã€‚

    Returns:
        Tuple[int, int]: æ’å…¥å’Œæ›´æ–°çš„å› å­æ•°é‡å…ƒç»„ã€‚

    Raises:
        RuntimeError: å¦‚æœæ•°æ®åº“æ“ä½œå¤±è´¥ã€‚

    è¯´æ˜:
        è¯¥å‡½æ•°ä¼šå°†å•é¡µå› å­æ•°æ®æ’å…¥æˆ–æ›´æ–°åˆ°æ•°æ®åº“ä¸­ã€‚
    """
    inserted_alphas: int = 0
    updated_alphas: int = 0

    try:
        async with get_db_session(Database.ALPHAS) as session:
            alpha_dal: AlphaDAL = AlphaDAL(session)

            # æ”¶é›†æ‰€æœ‰ competitions å’Œ classifications çš„ ID
            competition_ids: List[str] = [
                competition.id
                for alpha_data in alphas_results
                if alpha_data.competitions
                for competition in alpha_data.competitions
                if competition.id
            ]
            classification_ids: List[str] = [
                classification.id
                for alpha_data in alphas_results
                if alpha_data.classifications
                for classification in alpha_data.classifications
                if classification.id
            ]

            # æ‰¹é‡æŸ¥è¯¢ competitions å’Œ classifications
            competition_dal: CompetitionDAL = DALFactory.create_dal(
                CompetitionDAL, session
            )
            classification_dal: ClassificationDAL = DALFactory.create_dal(
                ClassificationDAL, session
            )

            competitions_dict: dict[str, Competition] = {
                competition.competition_id: competition
                for competition in await competition_dal.find_by(
                    in_={"competition_id": competition_ids}
                )
            }
            classifications_dict: dict[str, Classification] = {
                classification.classification_id: classification
                for classification in await classification_dal.find_by(
                    in_={"classification_id": classification_ids}
                )
            }

            for alpha_data in alphas_results:
                if exit_event.is_set():
                    await file_logger.awarning(
                        "æ£€æµ‹åˆ°é€€å‡ºäº‹ä»¶ï¼Œä¸­æ­¢å¤„ç†å› å­é¡µé¢", emoji="âš ï¸"
                    )
                    break
                alpha_id: str = alpha_data.id

                settings: Setting = create_alphas_settings(alpha_data)
                regular: Regular = create_alphas_regular(alpha_data.regular)

                # å¡«å…… classifications å’Œ competitions å­—æ®µ
                classifications: List[Classification] = [
                    classifications_dict[classification.id]
                    for classification in alpha_data.classifications or []
                    if classification.id in classifications_dict
                ]
                competitions: List[Competition] = [
                    competitions_dict[competition.id]
                    for competition in alpha_data.competitions or []
                    if competition.id in competitions_dict
                ]

                alpha: Alpha = create_alphas(
                    alpha_data, settings, regular, classifications, competitions
                )

                existing_alpha: Optional[Alpha] = await alpha_dal.find_by_alpha_id(
                    alpha_id
                )

                if existing_alpha:
                    alpha.id = existing_alpha.id
                    await alpha_dal.update(alpha)
                    updated_alphas += 1
                else:
                    await alpha_dal.create(alpha)
                    inserted_alphas += 1
    except Exception as e:
        raise RuntimeError(f"å¤„ç†å› å­é¡µé¢æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}") from e

    await file_logger.adebug(
        "å¤„ç†å› å­é¡µé¢æ•°æ®å®Œæˆ",
        inserted=inserted_alphas,
        updated=updated_alphas,
        emoji="âœ…",
    )
    return inserted_alphas, updated_alphas


async def process_alphas_for_date(
    client: WorldQuantClient, cur_time: datetime, parallel: int
) -> Tuple[int, int, int]:
    """
    åŒæ­¥å¤„ç†æŒ‡å®šæ—¥æœŸçš„ alphas æ•°æ®ï¼Œæ”¯æŒåˆ†ç‰‡å¹¶è¡Œå¤„ç†ã€‚

    Args:
        client: WorldQuantClient å®¢æˆ·ç«¯å®ä¾‹
        cur_time: æŒ‡å®šå¤„ç†çš„æ—¥æœŸ
        parallel: å¹¶è¡Œå¤„ç†ä»»åŠ¡æ•°

    Returns:
        Tuple[int, int, int]: è·å–ã€æ’å…¥å’Œæ›´æ–°çš„å› å­æ•°é‡å…ƒç»„
    """
    fetched_alphas: int = 0
    inserted_alphas: int = 0
    updated_alphas: int = 0

    # åˆå§‹åŒ–æ—¶é—´èŒƒå›´
    start_time: datetime = cur_time
    end_time: datetime = cur_time + timedelta(days=1)

    while start_time < cur_time + timedelta(days=1):
        query_params: SelfAlphaListQueryParams = SelfAlphaListQueryParams(
            limit=1,
            date_created_gt=start_time.isoformat(),
            date_created_lt=end_time.isoformat(),
        )
        alphas_data_result: Any
        alphas_data_result, _ = await client.alpha_get_self_list(query=query_params)

        if alphas_data_result.count < 10000:
            # ä½¿ç”¨æ­£ç¡®çš„å¼‚æ­¥æ—¥å¿—æ–¹æ³•
            await file_logger.ainfo(
                "è·å–æ—¥æœŸèŒƒå›´æ•°æ®",
                start_time=start_time,
                end_time=end_time,
                count=alphas_data_result.count,
                emoji="ğŸ“…",
            )

            # åˆ†ç‰‡å¤„ç†
            tasks: List[asyncio.Task] = []
            page_size: int = 100
            total_pages: int = (alphas_data_result.count + page_size - 1) // page_size
            pages_per_task: int = (total_pages + parallel - 1) // parallel

            for i in range(parallel):
                start_page: int = i * pages_per_task + 1
                end_page: int = min((i + 1) * pages_per_task, total_pages)
                if start_page > end_page:
                    break

                task: asyncio.Task = asyncio.create_task(
                    process_alphas_pages(
                        client,
                        start_time,
                        end_time,
                        start_page,
                        end_page,
                        page_size,
                    )
                )

                tasks.append(task)

            results: List[Tuple[int, int, int]] = await asyncio.gather(*tasks)
            for fetched, inserted, updated in results:
                fetched_alphas += fetched
                inserted_alphas += inserted
                updated_alphas += updated

            # æ›´æ–°æ—¶é—´èŒƒå›´ï¼Œç»§ç»­å¤„ç†åç»­æ—¶é—´æ®µ
            start_time = end_time
            end_time = cur_time + timedelta(days=1)
        else:
            # ç¼©å°æ—¶é—´èŒƒå›´
            mid_time: datetime = start_time + (end_time - start_time) / 2
            end_time = mid_time
            # ä½¿ç”¨æ­£ç¡®çš„å¼‚æ­¥æ—¥å¿—æ–¹æ³•
            await file_logger.ainfo(
                "æ•°æ®é‡è¶…è¿‡é™åˆ¶ï¼Œç¼©å°æ—¥æœŸèŒƒå›´",
                start_time=start_time,
                end_time=end_time,
                emoji="âš ï¸",
            )

    return fetched_alphas, inserted_alphas, updated_alphas


async def process_alphas_pages(
    client: WorldQuantClient,
    start_time: datetime,
    end_time: datetime,
    start_page: int,
    end_page: int,
    page_size: int,
) -> Tuple[int, int, int]:
    """
    å¤„ç†æŒ‡å®šé¡µèŒƒå›´å†…çš„ alphas æ•°æ®ã€‚

    Args:
        client: WorldQuantClient å®¢æˆ·ç«¯å®ä¾‹
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        start_page: èµ·å§‹é¡µç 
        end_page: ç»“æŸé¡µç 
        page_size: æ¯é¡µå¤§å°

    Returns:
        Tuple[int, int, int]: è·å–ã€æ’å…¥å’Œæ›´æ–°çš„å› å­æ•°é‡å…ƒç»„
    """
    fetched_alphas: int = 0
    inserted_alphas: int = 0
    updated_alphas: int = 0

    for page in range(start_page, end_page + 1):
        if exit_event.is_set():
            await file_logger.awarning("æ£€æµ‹åˆ°é€€å‡ºäº‹ä»¶ï¼Œä¸­æ­¢å¤„ç†å› å­é¡µèŒƒå›´", emoji="âš ï¸")
            break
        query_params: SelfAlphaListQueryParams = SelfAlphaListQueryParams(
            limit=page_size,
            offset=(page - 1) * page_size,
            date_created_gt=start_time.isoformat(),
            date_created_lt=end_time.isoformat(),
            order="dateCreated",
        )

        alphas_data_result: Any
        alphas_data_result, _ = await client.alpha_get_self_list(query=query_params)

        if not alphas_data_result.results:
            break

        fetched_alphas += len(alphas_data_result.results)
        # ä½¿ç”¨æ­£ç¡®çš„å¼‚æ­¥æ—¥å¿—æ–¹æ³•
        await file_logger.ainfo(
            "è·å–å› å­é¡µé¢æ•°æ®",
            start_time=start_time,
            end_time=end_time,
            page=page,
            count=len(alphas_data_result.results),
            emoji="ğŸ”",
        )

        inserted, updated = await process_alphas_page(alphas_data_result.results)
        inserted_alphas += inserted
        updated_alphas += updated

    return fetched_alphas, inserted_alphas, updated_alphas


async def fetch_and_process_alphas(
    client: WorldQuantClient,
    start_time: datetime,
    end_time: datetime,
    parallel: int,
) -> Tuple[int, int, int]:
    """
    è·å–å¹¶å¤„ç†æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„å› å­æ•°æ®ã€‚

    Args:
        client: WorldQuantClient å®¢æˆ·ç«¯å®ä¾‹
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        parallel: å¹¶è¡Œå¤„ç†ä»»åŠ¡æ•°

    Returns:
        Tuple[int, int, int]: è·å–ã€æ’å…¥å’Œæ›´æ–°çš„å› å­æ•°é‡å…ƒç»„
    """
    fetched_alphas: int = 0
    inserted_alphas: int = 0
    updated_alphas: int = 0

    for cur_time in (
        start_time + timedelta(days=i) for i in range((end_time - start_time).days + 1)
    ):
        if exit_event.is_set():
            await file_logger.awarning(
                "æ£€æµ‹åˆ°é€€å‡ºäº‹ä»¶ï¼Œä¸­æ­¢å› å­åŒæ­¥",
                current_date=cur_time,
                module=__name__,
                emoji="âš ï¸",
            )
            break

        fetched, inserted, updated = await process_alphas_for_date(
            client, cur_time, parallel
        )
        fetched_alphas += fetched
        inserted_alphas += inserted
        updated_alphas += updated

        # æ·»åŠ è°ƒè¯•æ—¥å¿—è¾“å‡ºåŒæ­¥è¿›åº¦
        await file_logger.adebug(
            "åŒæ­¥è¿›åº¦æ›´æ–°",
            current_date=cur_time,
            fetched=fetched_alphas,
            inserted=inserted_alphas,
            updated=updated_alphas,
            module=__name__,
            emoji="ğŸ“Š",
        )

    return fetched_alphas, inserted_alphas, updated_alphas


def setup_exit_signal_handler() -> None:
    """
    è®¾ç½®é€€å‡ºä¿¡å·å¤„ç†å™¨ã€‚

    åœ¨æ¥æ”¶åˆ°é€€å‡ºä¿¡å·æ—¶ï¼Œæ‰§è¡Œèµ„æºæ¸…ç†æ“ä½œå¹¶é€šçŸ¥åç¨‹é€€å‡ºã€‚
    """

    def handle_exit_signal(signum: int, _: Optional[types.FrameType]) -> None:
        file_logger.warning(
            "æ¥æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œå‡†å¤‡ç»ˆæ­¢æ“ä½œ",
            signal=signum,
            module=__name__,
            emoji="ğŸ›‘",
        )
        # è®¾ç½®é€€å‡ºäº‹ä»¶ï¼Œé€šçŸ¥æ‰€æœ‰åç¨‹åœæ­¢æ“ä½œ
        exit_event.set()

    signal.signal(signal.SIGINT, handle_exit_signal)  # å¤„ç† Ctrl+C
    signal.signal(signal.SIGTERM, handle_exit_signal)  # å¤„ç†ç»ˆæ­¢ä¿¡å·


async def sync_alphas(
    start_time: Optional[datetime] = None,
    end_time: Optional[datetime] = None,
    increamental: bool = False,
    parallel: int = 1,
) -> None:
    """
    å¼‚æ­¥åŒæ­¥å› å­ã€‚

    Args:
        start_time: å¼€å§‹æ—¶é—´ã€‚
        end_time: ç»“æŸæ—¶é—´ã€‚
        increamental: æ˜¯å¦å¢é‡åŒæ­¥ã€‚
        parallel: å¹¶è¡Œä»»åŠ¡æ•°ã€‚

    Raises:
        ValueError: å½“å¼€å§‹æ—¶é—´æ™šäºæˆ–ç­‰äºç»“æŸæ—¶é—´æ—¶æŠ›å‡º
    """
    if increamental:
        async with wq_client:
            sync_time_range: Tuple[datetime, datetime] = (
                await fetch_last_sync_time_range(wq_client)
            )

            start_time = (
                max(sync_time_range[0], start_time)
                if start_time
                else sync_time_range[0]
            )
            end_time = (
                min(sync_time_range[1], end_time) if end_time else sync_time_range[1]
            )
    else:
        if start_time is None:
            start_time = datetime.now() - timedelta(days=1)
        if end_time is None:
            end_time = datetime.now()

    if start_time >= end_time:
        raise ValueError("start_time å¿…é¡»æ—©äº end_timeã€‚")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦åŒæ­¥ç«èµ›æ•°æ®
    if competition_data_expire_check():
        await file_logger.ainfo(
            "ç«èµ›æ•°æ®è¿‡æœŸï¼Œå‡†å¤‡åŒæ­¥",
            start_time=start_time,
            end_time=end_time,
            emoji="ğŸ› ï¸",
        )
        await sync_competition()
        await file_logger.ainfo(
            "ç«èµ›æ•°æ®åŒæ­¥å®Œæˆ",
            start_time=start_time,
            end_time=end_time,
            emoji="âœ…",
        )

    # è®¾ç½®é€€å‡ºä¿¡å·å¤„ç†å™¨
    setup_exit_signal_handler()

    # ä½¿ç”¨æ­£ç¡®çš„å¼‚æ­¥æ—¥å¿—æ–¹æ³•
    await file_logger.ainfo("å¼€å§‹åŒæ­¥å› å­", emoji="ğŸš€")

    async with wq_client:
        try:
            fetched_alphas, inserted_alphas, updated_alphas = (
                await fetch_and_process_alphas(
                    wq_client, start_time, end_time, parallel
                )
            )

            # ä½¿ç”¨æ­£ç¡®çš„å¼‚æ­¥æ—¥å¿—æ–¹æ³•
            await file_logger.ainfo(
                "å› å­åŒæ­¥å®Œæˆ",
                fetched=fetched_alphas,
                inserted=inserted_alphas,
                updated=updated_alphas,
                module=__name__,
                emoji="âœ…",
            )
        except Exception as e:
            # ä½¿ç”¨æ­£ç¡®çš„å¼‚æ­¥æ—¥å¿—æ–¹æ³•
            await file_logger.aerror(
                "åŒæ­¥å› å­æ—¶å‡ºé”™",
                error=str(e),
                exc_info=True,
                module=__name__,
                emoji="âŒ",
            )
        finally:
            if exit_event.is_set():
                await file_logger.ainfo(
                    "å› å­åŒæ­¥è¢«ä¸­æ­¢",
                    fetched=fetched_alphas,
                    inserted=inserted_alphas,
                    updated=updated_alphas,
                    module=__name__,
                    emoji="ğŸ›‘",
                )
